{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "9GGeGewcJ-Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/우동협/공부/논문구현/RNNmodel/DB.json\""
      ],
      "metadata": {
        "id": "3xpTWDQ-J1Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, \"r\") as file:\n",
        "  data = json.load(file)\n",
        "print(type(data))\n",
        "print(data.keys())\n",
        "\n",
        "df_train = data['train']\n",
        "df_val = data['val']\n",
        "df_test = data['test']\n",
        "print(type(df_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErLAjdKhJ14V",
        "outputId": "dfba0834-fe51-43f2-b37a-2fd96bd0ac8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "dict_keys(['train', 'val', 'test'])\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 리스트 데이터를 PyTorch Tensor로 변환\n",
        "def preprocess_data(data):\n",
        "    X = torch.tensor([[d[\"return\"], d[\"S&P500_return\"]] for d in data[:-1]], dtype=torch.float32)\n",
        "    y = torch.tensor([d[\"return\"] for d in data[1:]], dtype=torch.float32)\n",
        "    # 입력 데이터를 torch 형태로 변환!\n",
        "    # 리스트, 이중 리스트, 다차원 리스트, NumPy 배열을 입력하면 tensor형태로 변환한다.\n",
        "\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Train, Val, Test 데이터 변환\n",
        "X_train, y_train = preprocess_data(df_train)\n",
        "X_val, y_val = preprocess_data(df_val)\n",
        "X_test, y_test = preprocess_data(df_test)\n",
        "\n",
        "print(\"Train X:\", X_train.shape, \"y:\", y_train.shape)\n",
        "print(\"Val X:\", X_val.shape, \"y:\", y_val.shape)\n",
        "print(\"Test X:\", X_test.shape, \"y:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boQNIK4KRdls",
        "outputId": "7fd50048-0758-41c6-b363-728fbfa20570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train X: torch.Size([1665, 2]) y: torch.Size([1665])\n",
            "Val X: torch.Size([204, 2]) y: torch.Size([204])\n",
            "Test X: torch.Size([104, 2]) y: torch.Size([104])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터를 데이터로더 처리"
      ],
      "metadata": {
        "id": "XVcr9T2zU3oZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 개념\n",
        "### Dataset\n",
        "- __getitem__()을 이용해 데이터를 한 개씩 불러오기\n",
        "- __len__()을 이용해 데이터 전체 크기 확인\n",
        "- RNN 같은 시계열 모델에서는 시퀀스 형태의 데이터도 만들 수 있음\n",
        "<br>\n",
        "✅ 하지만 PyTorch의 DataLoader는 Dataset을 상속받은 객체를 필요로 해.<br>\n",
        "✅ 위의 CustomDataset을 DataLoader에 넣으면 오류가 발생해.\n",
        "\n",
        "### DataLoader\n",
        "- 배치 학습(Batch Training) 지원 → 한 번에 여러 개의 데이터를 불러와 학습 속도를 높임.\n",
        "- shuffle=True → 데이터를 랜덤하게 섞어서 학습 가능 (과적합 방지)\n",
        "- num_workers → 데이터 로딩을 여러 개의 프로세스로 나눠서 빠르게 처리 가능"
      ],
      "metadata": {
        "id": "S2NvtjAKZwoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Dataset: 데이터를 정의하고 관리하는 클래스\n",
        "# DataLoader: Dataset에서 데이터를 배치(batch) 단위로 불러오는 도구\n",
        "\n",
        "\n",
        "class ReturnDataset(Dataset):\n",
        "    def __init__(self, X, y, sequence_length=5):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.X[idx : idx + self.sequence_length],  # 시퀀스 입력\n",
        "            self.y[idx + self.sequence_length - 1]  # 타겟 값\n",
        "        )\n",
        "    # (sequence_length, input_dim) : 데이터 단위 -> 해당 크기로 데이터를 전달\n",
        "    # X_sample.shape = (5, 2)  # (시퀀스 길이, 입력 차원)\n",
        "    # y_sample.shape = (1,)    # (예측할 값)\n",
        "\n",
        "# Dataset 생성\n",
        "sequence_length = 5\n",
        "train_dataset = ReturnDataset(X_train, y_train, sequence_length)\n",
        "val_dataset = ReturnDataset(X_val, y_val, sequence_length)\n",
        "test_dataset = ReturnDataset(X_test, y_test, sequence_length)\n",
        "\n",
        "# DataLoader 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)  # 검증 데이터는 순서 유지\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # 테스트 데이터도 순서 유지\n",
        "\n",
        "# DataLoader는 배치 크기로 데이터를 가져온다.\n",
        "# X_batch.shape = (16, sequence_length, input_dim)  # (배치 크기, 시퀀스 길이, 입력 차원)\n",
        "# y_batch.shape = (16,)  # (배치 크기)\n"
      ],
      "metadata": {
        "id": "PH8uwdTUU2mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfQ23MjFXgJJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=True):\n",
        "        super(CustomLSTM, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        # 가중치 초기화: 입력 → 게이트들 (input, forget, cell, output)\n",
        "        self.W_ih = nn.ParameterList([nn.Parameter(torch.randn(4 * hidden_size, input_size if i == 0 else hidden_size)) for i in range(num_layers)])\n",
        "        # 입력 데이터를 4개의 데이터로 나눈다. 신경망을 거치기 때문에 신경망 처리 완료!\n",
        "        # 4개의 데이터가 향하는 곳 : forget gate, input gate, cell state, output gate\n",
        "\n",
        "        self.W_hh = nn.ParameterList([nn.Parameter(torch.randn(4 * hidden_size, hidden_size)) for _ in range(num_layers)])\n",
        "        # 지난 시점의 hidden state 역시 4개의 데이터로 나뉜다.\n",
        "\n",
        "        # 편향 사용 유무에 따라 bias를 설계\n",
        "        if bias:\n",
        "            self.b_ih = nn.ParameterList([nn.Parameter(torch.randn(4 * hidden_size)) for _ in range(num_layers)])\n",
        "            self.b_hh = nn.ParameterList([nn.Parameter(torch.randn(4 * hidden_size)) for _ in range(num_layers)])\n",
        "        else:\n",
        "            self.register_parameter('b_ih', None)\n",
        "            self.register_parameter('b_hh', None)\n",
        "\n",
        "    def forward(self, x, states=None):\n",
        "        if self.batch_first:\n",
        "            x = x.transpose(0, 1)  # (batch, seq_len, input_size) → (seq_len, batch, input_size)\n",
        "\n",
        "        seq_len, batch_size, _ = x.size()\n",
        "\n",
        "        # t=0 일 때, hidden state와 cell state\n",
        "        if states is None:\n",
        "            h_t = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
        "            # (num_layers, batch_size, hidden) 형태의 데이터\n",
        "            c_t = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
        "        else:\n",
        "            h_t, c_t = states\n",
        "\n",
        "        output = []\n",
        "\n",
        "        for t in range(seq_len):  # 모든 timestep에 대해 계산\n",
        "            h_new, c_new = [], []\n",
        "            for layer in range(self.num_layers):\n",
        "                x_t = x[t] if layer == 0 else h_new[layer - 1]\n",
        "\n",
        "                gates = (x_t @ self.W_ih[layer].T + self.b_ih[layer] + h_t[layer] @ self.W_hh[layer].T + self.b_hh[layer])\n",
        "\n",
        "                i, f, g, o = gates.chunk(4, dim=1)  # 4개 게이트로 나누기\n",
        "                # (batch, hidden * 4)라 dim = 1로 나누면 각 배치 즉 개별 데이터의 형태는 유지되면서 4개의 데이터로 분할 가능\n",
        "\n",
        "                i, f, o = torch.sigmoid(i), torch.sigmoid(f), torch.sigmoid(o)\n",
        "                # input, forget, output gate는 sigmoid 함수를 통해 0~1 사이의 가중치 값으로 변경\n",
        "                g = torch.tanh(g)\n",
        "                # 입력데이터와 t-1 시점의 hidden state 정보가 더해진 데이터, tanh 함수를 통해 비선형성 추가\n",
        "\n",
        "                c_new_layer = f * c_t[layer] + i * g\n",
        "                # 해당 층의 cell state의 정보 중 불필요한 정보는 forget gate를 통해 제거\n",
        "                # (과거+새로운)정보를 input gate와 더하여 새로운 정보를 얼마나 반영할지 결정\n",
        "\n",
        "\n",
        "                h_new_layer = o * torch.tanh(c_new_layer)\n",
        "                # c_new_layer 과거의 정보와 현재의 정보가 더해진 정보\n",
        "                # output gate를 통해 얼마나 출력할지 결정\n",
        "                # (batch, hidden)\n",
        "\n",
        "                h_new.append(h_new_layer)\n",
        "                # 각 층의 hidden state가 담겨 있다.\n",
        "                c_new.append(c_new_layer)\n",
        "                # 각 층의  cell state가 담겨 있다.\n",
        "\n",
        "            output.append(h_new[-1])\n",
        "            # 각 시점의 마지막 hiddent state 값을 저장한다.\n",
        "            h_t, c_t = h_new, c_new\n",
        "            # 지난 시점의 각 층의 hidden state와 cell state 전달\n",
        "\n",
        "        output = torch.stack(output)  # (seq_len, batch_size, hidden_size)\n",
        "        # 기존에는 (batch_size, hidden_size) 값들이 리스트에 시간 순으로 담겨 있었다.\n",
        "        # 이를 하나의 텐서로 합치는 역할\n",
        "\n",
        "        if self.batch_first:\n",
        "            output = output.transpose(0, 1)  # (batch, seq_len, hidden_size)\n",
        "\n",
        "        return output, (torch.stack(h_t), torch.stack(c_t))  # 최종 hidden/cell state 반환\n",
        "\n",
        "        # output 각 시점이 마지막 hidden state값\n",
        "        # (torch.stack(h_t), torch.stack(c_t)) 마지막 시점의 모든 층의 hidden state와 cell state 반환"
      ],
      "metadata": {
        "id": "j7xMYWkiXlDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델 학습"
      ],
      "metadata": {
        "id": "CBf1anD4ZSF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(MyLSTM, self).__init__()\n",
        "        self.lstm = CustomLSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)  # 최종 예측 레이어\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # 마지막 타임스텝의 출력 사용\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "sC7kFTf7ZRuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화\n",
        "model = MyLSTM(input_size=2, hidden_size=10, num_layers=2, output_size=1)\n",
        "\n",
        "# 손실 함수 및 옵티마이저\n",
        "criterion = nn.MSELoss()  # 회귀 문제\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "CqCTwrosZVOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # 학습 모드\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch).squeeze()  # (batch, 1) → (batch,)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # 검증 단계\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            outputs = model(X_batch).squeeze()\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrYE9zeXZcfp",
        "outputId": "41473fdb-6580-4352-da7c-bc54e155bc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 0.3710, Val Loss: 1.7472\n",
            "Epoch [2/10], Train Loss: 0.1501, Val Loss: 1.3812\n",
            "Epoch [3/10], Train Loss: 0.1115, Val Loss: 1.2065\n",
            "Epoch [4/10], Train Loss: 0.0964, Val Loss: 1.1611\n",
            "Epoch [5/10], Train Loss: 0.0867, Val Loss: 1.1005\n",
            "Epoch [6/10], Train Loss: 0.0814, Val Loss: 1.0048\n",
            "Epoch [7/10], Train Loss: 0.0794, Val Loss: 0.9419\n",
            "Epoch [8/10], Train Loss: 0.0759, Val Loss: 0.9513\n",
            "Epoch [9/10], Train Loss: 0.0718, Val Loss: 0.8652\n",
            "Epoch [10/10], Train Loss: 0.0695, Val Loss: 0.7901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch).squeeze()\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "print(f\"Final Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRJl9zncZe3e",
        "outputId": "716c340d-69d1-41e9-8bf0-bc01604e84c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Loss: 0.0396\n"
          ]
        }
      ]
    }
  ]
}