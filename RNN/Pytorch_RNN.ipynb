{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "9GGeGewcJ-Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/우동협/공부/논문구현/RNNmodel/DB.json\""
      ],
      "metadata": {
        "id": "3xpTWDQ-J1Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, \"r\") as file:\n",
        "  data = json.load(file)\n",
        "print(type(data))\n",
        "print(data.keys())\n",
        "\n",
        "df_train = data['train']\n",
        "df_val = data['val']\n",
        "df_test = data['test']\n",
        "print(type(df_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErLAjdKhJ14V",
        "outputId": "cb7820d5-84d2-4cb7-92be-e9626ac509c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "dict_keys(['train', 'val', 'test'])\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 리스트 데이터를 PyTorch Tensor로 변환\n",
        "def preprocess_data(data):\n",
        "    X = torch.tensor([[d[\"return\"], d[\"S&P500_return\"]] for d in data[:-1]], dtype=torch.float32)\n",
        "    y = torch.tensor([d[\"return\"] for d in data[1:]], dtype=torch.float32)\n",
        "    # 입력 데이터를 torch 형태로 변환!\n",
        "    # 리스트, 이중 리스트, 다차원 리스트, NumPy 배열을 입력하면 tensor형태로 변환한다.\n",
        "\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Train, Val, Test 데이터 변환\n",
        "X_train, y_train = preprocess_data(df_train)\n",
        "X_val, y_val = preprocess_data(df_val)\n",
        "X_test, y_test = preprocess_data(df_test)\n",
        "\n",
        "print(\"Train X:\", X_train.shape, \"y:\", y_train.shape)\n",
        "print(\"Val X:\", X_val.shape, \"y:\", y_val.shape)\n",
        "print(\"Test X:\", X_test.shape, \"y:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boQNIK4KRdls",
        "outputId": "e5d4d956-e8cc-4537-e766-30dd4509ba12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train X: torch.Size([1665, 2]) y: torch.Size([1665])\n",
            "Val X: torch.Size([204, 2]) y: torch.Size([204])\n",
            "Test X: torch.Size([104, 2]) y: torch.Size([104])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터를 데이터로더 처리"
      ],
      "metadata": {
        "id": "XVcr9T2zU3oZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 개념\n",
        "### Dataset\n",
        "- __getitem__()을 이용해 데이터를 한 개씩 불러오기\n",
        "- __len__()을 이용해 데이터 전체 크기 확인\n",
        "- RNN 같은 시계열 모델에서는 시퀀스 형태의 데이터도 만들 수 있음\n",
        "<br>\n",
        "✅ 하지만 PyTorch의 DataLoader는 Dataset을 상속받은 객체를 필요로 해.<br>\n",
        "✅ 위의 CustomDataset을 DataLoader에 넣으면 오류가 발생해.\n",
        "\n",
        "### DataLoader\n",
        "- 배치 학습(Batch Training) 지원 → 한 번에 여러 개의 데이터를 불러와 학습 속도를 높임.\n",
        "- shuffle=True → 데이터를 랜덤하게 섞어서 학습 가능 (과적합 방지)\n",
        "- num_workers → 데이터 로딩을 여러 개의 프로세스로 나눠서 빠르게 처리 가능"
      ],
      "metadata": {
        "id": "S2NvtjAKZwoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "# Dataset: 데이터를 정의하고 관리하는 클래스\n",
        "# DataLoader: Dataset에서 데이터를 배치(batch) 단위로 불러오는 도구\n",
        "\n",
        "\n",
        "class ReturnDataset(Dataset):\n",
        "    def __init__(self, X, y, sequence_length=5):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (\n",
        "            self.X[idx : idx + self.sequence_length],  # 시퀀스 입력\n",
        "            self.y[idx + self.sequence_length - 1]  # 타겟 값\n",
        "        )\n",
        "    # (sequence_length, input_dim) : 데이터 단위 -> 해당 크기로 데이터를 전달\n",
        "    # X_sample.shape = (5, 2)  # (시퀀스 길이, 입력 차원)\n",
        "    # y_sample.shape = (1,)    # (예측할 값)\n",
        "\n",
        "# Dataset 생성\n",
        "sequence_length = 5\n",
        "train_dataset = ReturnDataset(X_train, y_train, sequence_length)\n",
        "val_dataset = ReturnDataset(X_val, y_val, sequence_length)\n",
        "test_dataset = ReturnDataset(X_test, y_test, sequence_length)\n",
        "\n",
        "# DataLoader 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)  # 검증 데이터는 순서 유지\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # 테스트 데이터도 순서 유지\n",
        "\n",
        "# DataLoader는 배치 크기로 데이터를 가져온다.\n",
        "# X_batch.shape = (16, sequence_length, input_dim)  # (배치 크기, 시퀀스 길이, 입력 차원)\n",
        "# y_batch.shape = (16,)  # (배치 크기)\n"
      ],
      "metadata": {
        "id": "PH8uwdTUU2mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdN-pvyDdCVp",
        "outputId": "a6ff3b63-20c2-4948-9917-05d4ae0cc8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1752,  3.0118, -0.6022, -0.3720, -0.3000,  0.0476, -0.3295,  0.1542,\n",
              "        -0.1954, -0.1083, -0.2878, -0.5557, -0.3762, -1.0277, -0.3155, -0.2977])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델"
      ],
      "metadata": {
        "id": "2i9JLW5hJ1kq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2EF2WzOWhgd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MyRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(MyRNN, self).__init__()\n",
        "        self.rnn = CustomRNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # nn 모듈에서 RNN 불러오기\n",
        "        self.fc = nn.Linear(hidden_size, output_size)  # 최종 출력층\n",
        "        # 원하는 task에 맞게 차원을 변경할 수 있는 레이어\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, hidden = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])  # 마지막 타임스텝의 출력 사용\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 모델 초기화 (입력 크기 = 2, 은닉 크기 = 16, 층 개수 = 2, 출력 크기 = 1)\n",
        "model = MyRNN(input_size=2, hidden_size=10, num_layers=2, output_size=1)\n",
        "\n",
        "# 손실 함수 및 옵티마이저\n",
        "criterion = nn.MSELoss()  # 회귀 문제이므로 MSELoss 사용\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # 학습 모드\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch).squeeze()  # (batch, 1) → (batch,)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # 검증 단계 (Val 데이터 활용)\n",
        "    model.eval()  # 평가 모드\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            outputs = model(X_batch).squeeze()\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtNywYq0YL1W",
        "outputId": "775034c2-a2a0-4bef-a8ff-8d5a7c7edae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Loss: 0.5170, Val Loss: 2.0776\n",
            "Epoch [2/10], Train Loss: 0.2300, Val Loss: 1.6095\n",
            "Epoch [3/10], Train Loss: 0.1699, Val Loss: 1.4942\n",
            "Epoch [4/10], Train Loss: 0.1335, Val Loss: 1.3099\n",
            "Epoch [5/10], Train Loss: 0.1172, Val Loss: 1.2961\n",
            "Epoch [6/10], Train Loss: 0.1099, Val Loss: 1.2747\n",
            "Epoch [7/10], Train Loss: 0.1077, Val Loss: 1.2151\n",
            "Epoch [8/10], Train Loss: 0.1109, Val Loss: 1.1964\n",
            "Epoch [9/10], Train Loss: 0.1081, Val Loss: 1.1627\n",
            "Epoch [10/10], Train Loss: 0.0974, Val Loss: 1.0815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 평가\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        outputs = model(X_batch).squeeze()\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "print(f\"Final Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHN-wqjWVSLX",
        "outputId": "843a318f-9054-4910-dd06-6e4f1fb6a19c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Loss: 0.0890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN github 코드 살펴보기"
      ],
      "metadata": {
        "id": "HR_1RoWTitd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomRNN(nn.Module):\n",
        "    def __init__(\n",
        "        self, input_size, hidden_size, num_layers=1, nonlinearity=\"tanh\", bias=True, batch_first=True, dropout=0.0, bidirectional=False\n",
        "    ):\n",
        "        super(CustomRNN, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        # 입력 벡터의 크기\n",
        "        self.hidden_size = hidden_size\n",
        "        # RNN 모델이 데이터를 처리할 때, Hidden state의 크기\n",
        "        self.num_layers = num_layers\n",
        "        # RNN 신경망을 위로 몇 층 쌓을지 결정\n",
        "        self.nonlinearity = nonlinearity\n",
        "        # 사용할 비선형 함수, tanh 함수와 ReLu 함수가 있다.\n",
        "        self.bias = bias\n",
        "        # 편향 사용 유무\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        if nonlinearity == \"tanh\":\n",
        "            self.activation = torch.tanh\n",
        "        elif nonlinearity == \"relu\":\n",
        "            self.activation = torch.relu\n",
        "        else:\n",
        "            raise ValueError(\"Invalid nonlinearity. Choose either 'tanh' or 'relu'\")\n",
        "\n",
        "        # 가중치 및 바이어스 초기화\n",
        "        self.weight_ih = nn.ParameterList([nn.Parameter(torch.randn(hidden_size, input_size if i == 0 else hidden_size)) for i in range(num_layers)])\n",
        "        # nn.ParameterList([...])\n",
        "        # torch.nn.Module에서 학습 가능한 파라미터 목록을 만들기 위해 사용됩니다.\n",
        "        # 내부의 nn.Parameter들이 학습 가능한 텐서로 등록됩니다.\n",
        "\n",
        "        # nn.Parameter()\n",
        "        # 학습 가능한 텐서(가중치, 편향 등)를 선언할 때 사용.\n",
        "        # requires_grad=True가 기본값으로 설정되어 있어서, 역전파(Gradient Descent) 과정에서 자동으로 업데이트됨.\n",
        "        # nn.Parameter를 사용하면 model.parameters() 호출 시 자동으로 포함\n",
        "        # nn.ParameterList와 함께 사용하여 여러 개의 가중치를 쉽게 관리 가능\n",
        "\n",
        "        self.weight_hh = nn.ParameterList([nn.Parameter(torch.randn(hidden_size, hidden_size)) for _ in range(num_layers)])\n",
        "\n",
        "        if bias:\n",
        "            self.bias_ih = nn.ParameterList([nn.Parameter(torch.randn(hidden_size)) for _ in range(num_layers)])\n",
        "            self.bias_hh = nn.ParameterList([nn.Parameter(torch.randn(hidden_size)) for _ in range(num_layers)])\n",
        "        else:\n",
        "            self.register_parameter('bias_ih', None)\n",
        "            self.register_parameter('bias_hh', None)\n",
        "\n",
        "            # PyTorch의 nn.Module에서 self.register_parameter(name, param) 함수는 모델의 학습 가능한 매개변수(파라미터)를 등록하는 역할\n",
        "            # 'bias_ih'라는 이름을 가진 학습 가능한 매개변수를 등록하지만, 실제 값은 None으로 설정.\n",
        "            # 즉, 이 RNN 모델에서는 bias_ih를 사용하지 않겠다는 의미\n",
        "\n",
        "    def forward(self, x, hx=None):\n",
        "      if self.batch_first:\n",
        "          x = x.transpose(0, 1)  # (batch, seq_len, input_size) -> (seq_len, batch, input_size)\n",
        "\n",
        "      seq_len, batch_size, _ = x.size()\n",
        "\n",
        "      if hx is None:\n",
        "          hx = torch.zeros(self.num_layers, batch_size, self.hidden_size, device=x.device)\n",
        "\n",
        "      h_prev = [hx[layer] for layer in range(self.num_layers)]  # 이전 hidden states 저장\n",
        "      output = []\n",
        "\n",
        "      for t in range(seq_len):\n",
        "          h_new = []  # 새로운 hidden states 저장할 리스트\n",
        "          for layer in range(self.num_layers):\n",
        "              if layer == 0:\n",
        "                  layer_input = x[t]  # 첫 번째 layer는 입력 데이터를 사용\n",
        "              else:\n",
        "                  layer_input = h_new[layer - 1]  # 이전 layer의 출력을 입력으로 사용\n",
        "\n",
        "              new_h = self.activation(\n",
        "                  layer_input @ self.weight_ih[layer].T + self.bias_ih[layer] +\n",
        "                  h_prev[layer] @ self.weight_hh[layer].T + self.bias_hh[layer]\n",
        "              )\n",
        "              h_new.append(new_h)  # 새로운 hidden state 저장\n",
        "\n",
        "          output.append(h_new[-1])  # 마지막 layer의 hidden state 저장\n",
        "          h_prev = h_new  # 현재 hidden states를 다음 타임스텝에서 사용\n",
        "\n",
        "      output = torch.stack(output)  # (seq_len, batch_size, hidden_size)\n",
        "\n",
        "      if self.batch_first:\n",
        "          output = output.transpose(0, 1)  # (batch, seq_len, hidden_size)\n",
        "\n",
        "      return output, torch.stack(h_prev)  # 마지막 hidden state도 반환\n"
      ],
      "metadata": {
        "id": "mtcKknKCivw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 1\n",
        "for i in range(num_layers):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7T0nNcIfU4l",
        "outputId": "dd30b7a9-14bf-4b8a-d9d8-1fcc77388ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    }
  ]
}